{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9d8c5-dbf8-495b-a97a-0aabdbd8d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Graphical library\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from math import sqrt\n",
    "\n",
    "from IPython.display import HTML, clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a6856c-c7fe-4876-b182-b3080e25d3b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Solution of Lab Assignment 4 :  \n",
    "See pdf for instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821e1ca-ff1c-4b0a-b4a2-360ab7dc8e8b",
   "metadata": {},
   "source": [
    "## Part 1: Continuous Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed18c856-569b-42c0-a94b-4568ba9687f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Match\n",
    "\n",
    "import numpy as np\n",
    "from gym import logger, spaces\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gym.utils import seeding\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "# Continuous cartpole class: this class modifies the original Gym class to be \n",
    "# able to handle continuous actions as input\n",
    "# You DO NOT need to understand it to work on this lab assessment\n",
    "\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "\n",
    "class ContinuousCartPoleEnv(CartPoleEnv):\n",
    "    def __init__(self, length=0.5, masspole=0.1, **kwords):\n",
    "        super().__init__(**kwords)\n",
    "        self.length = length\n",
    "        self.masspole = 0.1\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.steps_beyond_done = None\n",
    "    \n",
    "    def step(self, force):\n",
    "\n",
    "        err_msg = \"%r (%s) invalid\" % (force, type(force))\n",
    "        assert type(force)==float, err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, False, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        self.steps_beyond_done = None\n",
    "        return super().reset()\n",
    "\n",
    "gym.envs.register(\n",
    "     id='ContinuousCartPoleEnv',\n",
    "     entry_point=ContinuousCartPoleEnv,\n",
    "     max_episode_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bcfb52-1ac5-4832-a053-846aa1587a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 2: Deep Deterministic Policy Gradients (DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d63061-3a15-4606-836f-8ca78348d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, layer_tuple, output_activation='relu', max_output=1):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(layer_tuple[l], layer_tuple[l+1]) for l in range(len(layer_tuple)-1)])\n",
    "        self.hidden_activation = nn.ReLU()\n",
    "\n",
    "        self.max_output = max_output\n",
    "        self.output_activation = nn.Identity()\n",
    "        if output_activation == 'relu':\n",
    "            self.output_activation = nn.ReLU()\n",
    "        if output_activation == 'tanh':\n",
    "            self.output_activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.hidden_activation(layer(x))\n",
    "        \n",
    "        return self.max_output*self.output_activation(self.layers[-1](x))\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, layers, max_force=1, device='cpu'):\n",
    "\n",
    "        self.model = MLP(layers, output_activation='tanh', max_output=max_force).to(device)\n",
    "        self.optim = optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.target = deepcopy(self.model).to(device)\n",
    "\n",
    "    def optimise_step(self, critic, state):\n",
    "\n",
    "        #TODO\n",
    "\n",
    "    def update_target(self, tau):\n",
    "        for target_param, local_param in zip(self.target.parameters(), self.model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, layers, device='cpu'):\n",
    "\n",
    "        self.model = MLP(layers).to(device)\n",
    "        self.optim = optim.Adam(self.model.parameters())\n",
    "\n",
    "        self.target = deepcopy(self.model).to(device)\n",
    "        self.target.eval()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def optimise_step(self, actor, transitions, non_final_next_states, non_final_mask, GAMMA):\n",
    "        \n",
    "        #TODO\n",
    "\n",
    "    def update_target(self, tau):\n",
    "        for target_param, local_param in zip(self.target.parameters(), self.model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "\n",
    "# From OpenAI Baselines:\n",
    "# https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-3, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def noise(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt \\\n",
    "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec461d36-3ebf-489f-ae14-a30183a89e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the Actor and Critic models and other hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c6907-a411-4227-84ad-145007913dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1500\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 1.\n",
    "NOISE_START = 0.01\n",
    "NOISE_END = 0\n",
    "FINAL_NOISE_DECAY = 5   \n",
    "NOISE_DECAY = NUM_EPISODES/FINAL_NOISE_DECAY     #number of episodes for epsilon to decay by 1/e\n",
    "TAU = 0.001       #target update\n",
    "\n",
    "# Get number of states and actions from gym action space\n",
    "env = gym.make('ContinuousCartPoleEnv')\n",
    "env.reset()\n",
    "state_dim = len(env.state)    #x, x_dot, theta, theta_dot\n",
    "action_dim = 1\n",
    "\n",
    "##noise parameters\n",
    "ou_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim),\n",
    "                                            sigma=float(.3) * np.ones(action_dim))\n",
    "ou_noise.reset()\n",
    "\n",
    "hidden_layer_actor = 128\n",
    "hidden_layer_critic = 256\n",
    "actor_layers = (state_dim, hidden_layer_actor, action_dim)\n",
    "critic_layers = (state_dim + action_dim, hidden_layer_critic, 1)\n",
    "\n",
    "actor = Actor(actor_layers, max_force=1, device=device)\n",
    "critic = Critic(critic_layers, device=device)\n",
    "\n",
    "memory = ReplayMemory(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a12788-49a9-4d28-9d59-c29cc1436c51",
   "metadata": {},
   "source": [
    "### Optimisation function for the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcd48c-6475-4161-8240-3678d22605fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_models(actor, critic, memory, GAMMA=GAMMA, BATCH_SIZE=BATCH_SIZE):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "   \n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    critic.optimise_step(actor, (state_batch, action_batch, reward_batch), \n",
    "                         non_final_next_states, non_final_mask, GAMMA=GAMMA)\n",
    "    actor.optimise_step(critic, state_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c21a31-991f-471c-a184-34085319391e",
   "metadata": {},
   "source": [
    "### Training the Actor and Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249bd35-30fd-4223-adb1-673ba7fba386",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "actions_taken = []\n",
    "noises = []\n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    ep_actions = []\n",
    "    ep_noise = []\n",
    "    if i_episode % 20 == 0:\n",
    "        print(\"episode \", i_episode, \"/\", NUM_EPISODES)\n",
    "        \n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.state).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    noise_amplitude = NOISE_END + (NOISE_START - NOISE_END) * \\\n",
    "        math.exp(-1. * i_episode / NOISE_DECAY)\n",
    "    \n",
    "    for t in count():\n",
    "        noise = noise_amplitude*np.random.randn()\n",
    "        noise = torch.tensor(ou_noise.noise()).to(device) * 1.0/(1.0 + 0.1*i_episode)\n",
    "\n",
    "        actor_action = actor.model(state).flatten().detach().to(device)    #avoid backpropagating through the chosen action\n",
    "        action_taken = (actor_action + noise).float()\n",
    "\n",
    "        _, reward, terminated, truncated, _ = env.step(float(action_taken))\n",
    "        done = terminated or truncated\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Observe new state\n",
    "        if not done:\n",
    "            next_state = torch.tensor(env.state).float().unsqueeze(0).to(device)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store transition in memory\n",
    "        memory.push(state, action_taken, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Optimise actor and critic\n",
    "        optimise_models(actor, critic, memory)\n",
    "        \n",
    "        #Update target networks\n",
    "        actor.update_target(TAU)\n",
    "        critic.update_target(TAU)\n",
    "        \n",
    "        #End episode\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            if i_episode % 20 == 0:\n",
    "                print(\"duration  \", episode_durations[-1])\n",
    "            break\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ae4a9-837b-42e1-9045-251b05fb24ce",
   "metadata": {},
   "source": [
    "### Visualisation of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8627c6b-af2c-4512-b1e5-28be66f9b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve for one run\n",
    "\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"episode duration\")\n",
    "plt.plot(episode_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d3673-61a1-4ec0-9742-7b6391ade146",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "video_dir = os.path.join(cwd, 'trained_agent')\n",
    "if not os.path.isdir(video_dir):\n",
    "    os.mkdir(video_dir)\n",
    "video_file = os.path.join(video_dir, \"trained_agent.mp4\")\n",
    "\n",
    "env = gym.make('ContinuousCartPoleEnv', render_mode=\"rgb_array\")\n",
    "video_recorder = VideoRecorder(env, video_file, enabled=True) \n",
    "\n",
    "#Performing the episode\n",
    "state = env.reset()\n",
    "done = False\n",
    "state = torch.tensor(env.state).float()\n",
    "\n",
    "duration = 0\n",
    "\n",
    "while not done:\n",
    "\n",
    "    # Store the current state of the CartPole for the video\n",
    "    video_recorder.capture_frame()\n",
    "\n",
    "    # Predict the state with the model\n",
    "    action = actor.model(state)\n",
    "    _, reward, terminated, truncated, _ = env.step(float(action))\n",
    "    done = terminated or truncated\n",
    "    duration += 1\n",
    "    state = torch.tensor(env.state).float()\n",
    "\n",
    "\n",
    "video_recorder.capture_frame()\n",
    "video_recorder.close()\n",
    "video_recorder.enabled = False\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
