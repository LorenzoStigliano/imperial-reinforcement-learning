{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "id": "20IyxDzgp3tU",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Graphical library\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNPHHkRcUTLg"
   },
   "source": [
    "# Lab Assignment 3 :  \n",
    "See pdf for instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q06EldpwfoEB"
   },
   "source": [
    "## Part 1: Introduction to Gym environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pl3dm2KOPrfi"
   },
   "source": [
    "### Question 1: Creating the Cartpole environment and performing an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzdwlOmsfydj"
   },
   "outputs": [],
   "source": [
    "# Creating the environment and a recorder to save a video in the './random_episode' folder\n",
    "# To save multiple videos, save each mp4 fle to a new directory\n",
    "\n",
    "cwd = os.getcwd()\n",
    "video_dir = os.path.join(cwd, 'random_episode')\n",
    "if not os.path.isdir(video_dir):\n",
    "    os.mkdir(video_dir)\n",
    "video_file = os.path.join(video_dir, \"random_episode.mp4\")\n",
    "env = gym.make('CartPole-v0', render_mode=\"rgb_array\")\n",
    "\n",
    "# Perform an episode in the environemnt with random actions\n",
    "state, info = env.reset()\n",
    "\n",
    "video_recorder = VideoRecorder(env, video_file, enabled=True)  #record a video of the episode\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    video_recorder.capture_frame()\n",
    "    action = env.action_space.sample()  # sample a random possible action from the CartPole env\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "    \n",
    "video_recorder.capture_frame()\n",
    "video_recorder.close()\n",
    "video_recorder.enabled = False\n",
    "\n",
    "print(f\"Video saved in folder {video_dir}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG1q0h4MuOvp"
   },
   "source": [
    "### Question 2: Implementing a simple hand-designed policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9o3d_CI_Pon1"
   },
   "outputs": [],
   "source": [
    "# [Action required]\n",
    "def simple_policy(state, p_random):\n",
    "    \"\"\"\n",
    "    Simple hand-crafted policy to act in the Cartpole environment.\n",
    "    Input: \n",
    "        - state {tensor} - current state of the environment\n",
    "        - p_random {float} - probability that the action is random\n",
    "    Output: action {int} - action to perform in the environemnt\n",
    "    \"\"\"\n",
    "    #### \n",
    "    # Add your code here\n",
    "    ####\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ys68tHGEPh7u"
   },
   "outputs": [],
   "source": [
    "# Rate of random action sampling\n",
    "p_random = 0.2\n",
    "\n",
    "# Performing an episode in the environemnt with simple policy\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = simple_policy(state, p_random)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjtZS1qHUzHH"
   },
   "source": [
    "## Part 2: Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7Tox87FUXwD"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GmBYBmriPSZ"
   },
   "outputs": [],
   "source": [
    "# Graphical class: this class modifies the original Gym class to be able to visualise your prediction\n",
    "# You DO NOT need to understand it to work on this lab assessment\n",
    "\n",
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "from gym.wrappers.time_limit import TimeLimit\n",
    "\n",
    "class ShowCartPolePredictions(CartPoleEnv):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def step(self, state):\n",
    "        \"\"\"\n",
    "        Step takes the next state as input instead of action.\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "                )\n",
    "\n",
    "        reward = 1.\n",
    "        truncated = False\n",
    "            \n",
    "        return np.array(self.state, dtype=np.float32), reward, done, truncated, {}\n",
    "\n",
    "nb_path = os.path.join(os.getcwd(), 'tutorial_3_solution.ipynb')\n",
    "gym.envs.register(\n",
    "     id='ShowPredictionsCartPole',\n",
    "     entry_point=ShowCartPolePredictions,\n",
    "     max_episode_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKHIOPm_UwK4"
   },
   "source": [
    "### Question 3: Understanding the MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrrOYjZfcEIA"
   },
   "outputs": [],
   "source": [
    "# Multi Layer perceptron class\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, num_hidden, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialise the network.\n",
    "        Input:\n",
    "            - input_size {int} - size of input to the network\n",
    "            - output_size {int} - size of output to the network\n",
    "            - num_hidden {int} - number of hidden layers\n",
    "            - hidden_size {int} - size of each hidden layer\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size) # First tranformation from the network input to the input of first hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)]) # All the hidden transformation\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size) # Last tranformation from the last hidden layer output to the network output\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Get the output of the MLP.\n",
    "        Input: x {tensor} - one element or a batch of element\n",
    "        Ouput: y {tensor} - corresponding output\n",
    "        \"\"\"\n",
    "        x.to(device)\n",
    "        x = self.input_layer(x) # Passing through the input layer\n",
    "        x = F.relu(x) # Applying Relu activation\n",
    "        for layer in self.hidden_layers:\n",
    "          x = layer(x) # Passing through each hidden layer\n",
    "          x = F.relu(x) # Applying Relu activation\n",
    "        x = self.output_layer(x) # Passing through the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVEqgNkRwgCd",
    "outputId": "c6fe84c7-7a74-4343-d687-028af0c6a02e"
   },
   "outputs": [],
   "source": [
    "# Initialise an MLP instance\n",
    "input_size = 10\n",
    "output_size = 10\n",
    "num_hidden = 3\n",
    "hidden_size = 15\n",
    "\n",
    "model = MLP(input_size, output_size, num_hidden, hidden_size)\n",
    "\n",
    "# Creating some false input\n",
    "x = torch.rand(10) # Random tensor\n",
    "print(\"The input is:\\n\", x)\n",
    "\n",
    "# Passing it through the network\n",
    "y = model.forward(x)\n",
    "print(\"\\nThe correpsonding output is:\\n\", y)\n",
    "print(\"\\nThe network has not been trained yet so this output is random.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lBcIUZ1x6Bg"
   },
   "source": [
    "### Question 4: Collecting data to train the state-predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aydDmDSz3olA"
   },
   "outputs": [],
   "source": [
    "def batch_data(state_list, action_list, next_state_list, batch_size, num_batches):\n",
    "  \"\"\"\n",
    "  Reshape the data to match the model requirements.\n",
    "  Input:\n",
    "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
    "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
    "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
    "    - batch_size {int} - number of steps in a batch\n",
    "    - num_batches {int} - total number of batches\n",
    "  Ouput:\n",
    "    - batched_state_action {torch.tensor} - input of the model of size (batch_size, 5)\n",
    "    - batched_next_state {torch.tensor} - target output of the model of size (batch_size, 4)\n",
    "  \"\"\"\n",
    "  # Reshape and concatenate the state and action (input of the network)\n",
    "  state_action_list = [torch.cat((torch.tensor(state_list[i]).float().unsqueeze(0), torch.tensor(action_list[i]).unsqueeze(0).unsqueeze(0)), dim=-1) for i in range(len(state_list))]\n",
    "  state_action = torch.cat(state_action_list)\n",
    "\n",
    "  # Reshape the next state\n",
    "  next_state = torch.cat([torch.tensor(next_state_list[i]).float().unsqueeze(0) for i in range(len(next_state_list))])\n",
    "\n",
    "  # Rearrange the data into batches\n",
    "  batched_state_action = [state_action[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
    "  batched_next_state = [next_state[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
    "\n",
    "  return batched_state_action, batched_next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKfJjzV6UrfO"
   },
   "outputs": [],
   "source": [
    "# [Action required]\n",
    "def collect_data(num_episodes, p_random): \n",
    "  \"\"\"\n",
    "  Collect the data to train the predictor model.\n",
    "  Input:\n",
    "    - num_episode {int} - number of episodes to collect\n",
    "    - p_random {float} - probability used for the simple policy\n",
    "  Output:\n",
    "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
    "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
    "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
    "  \"\"\"\n",
    "\n",
    "  # Containers for the data\n",
    "  state_list = [] # List of current states\n",
    "  action_list = [] # List of current actions\n",
    "  next_state_list = [] # List of next step states\n",
    "\n",
    "  # Creating the environment\n",
    "  env = gym.make('CartPole-v0')\n",
    "\n",
    "  #### \n",
    "  # Add your code here\n",
    "  # This is an example on how to fill state_list, action_list and next_state_list\n",
    "  # You would need to update it to collect enough data\n",
    "  \n",
    "  state, info = env.reset()\n",
    "  action = simple_policy(state, p_random)\n",
    "  next_state, reward, done, truncated, info = env.step(action)\n",
    "  state_list.append(state)\n",
    "  action_list.append(action)\n",
    "  next_state_list.append(next_state)\n",
    "\n",
    "  ####\n",
    "  \n",
    "\n",
    "  # Closing the environment\n",
    "  env.close()\n",
    "\n",
    "  return state_list, action_list, next_state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnZ5-DHlyDnD"
   },
   "outputs": [],
   "source": [
    "# Define parameters for the model\n",
    "num_episodes = 5000 # Total number of episodes collected in our dataset\n",
    "batch_size = 128 # Size of the batch to train the DNN\n",
    "p_random = 0.2 # Parameter of the simple_policy\n",
    "\n",
    "# Collect the data\n",
    "state_list, action_list, next_state_list = collect_data(num_episodes, p_random)\n",
    "num_batches = int(len(state_list)/batch_size)\n",
    "\n",
    "# Reshape them to match the model input/output\n",
    "batched_state_action, batched_next_state = batch_data(state_list, action_list, next_state_list, batch_size, num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Bk-PIDvy3-o"
   },
   "source": [
    "### Question 5: Training a state predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-7uYOAw6_3c"
   },
   "outputs": [],
   "source": [
    "def MSE_loss(prediction, target):\n",
    "  \"\"\" \n",
    "  MSE loss function.\n",
    "  Input:\n",
    "    - prediction {torch.tensor} - target\n",
    "    - target {torch.tensor} - model prediction\n",
    "  Output: loss {float} - MSE error between the prediction and the target\n",
    "  \"\"\"\n",
    "  return ((prediction - target)**2).sum(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyeJfvwXp3ta",
    "new_sheet": false,
    "outputId": "563d675a-a0e1-4c0f-8146-8d07f7f1e36b",
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Defining the parameters\n",
    "state_dim = 4\n",
    "action_dim = 1\n",
    "\n",
    "input_size = state_dim + action_dim\n",
    "output_size = state_dim\n",
    "num_hidden = 2\n",
    "hidden_size = 50\n",
    "\n",
    "# Creating the predictor model\n",
    "state_predictor = MLP(input_size, output_size, num_hidden, hidden_size)\n",
    "\n",
    "# Creating the optmizer\n",
    "optimiser = optim.Adam(state_predictor.parameters())\n",
    "\n",
    "# [Action required]\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "losses = [] # Contain all successive loss function values\n",
    "\n",
    "#### \n",
    "# Add your code here\n",
    "#### \n",
    "\n",
    "# Closing the environment\n",
    "env.close()\n",
    "\n",
    "# Plot the loss across training\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sK7wN8a9GKC"
   },
   "outputs": [],
   "source": [
    "# Displaying the learned model dynamics in the CartPole environment\n",
    "\n",
    "# simulated_env allows us to visualise the learned model dynamics\n",
    "# by calling simulated_env.set_next_state(next_state) we set the learned\n",
    "# next_state and we can visualise what the learned dynamics looks like\n",
    "# video saved in the 'learned_dynamics' folder\n",
    "\n",
    "# setting up the visualisation \n",
    "\n",
    "cwd = os.getcwd()\n",
    "video_dir = os.path.join(cwd, 'learned_dynamics')\n",
    "if not os.path.isdir(video_dir):\n",
    "    os.mkdir(video_dir)\n",
    "video_file = os.path.join(video_dir, \"learned_dynamics.mp4\")\n",
    "\n",
    "simulated_env = gym.make('ShowPredictionsCartPole')\n",
    "#TimeLimit)(ShowCartPolePredictions(), max_episode_steps=500)\n",
    "video_recorder = VideoRecorder(simulated_env, video_file, enabled=True) \n",
    "\n",
    "state = simulated_env.reset()\n",
    "\n",
    "#Performing the episode\n",
    "state, info = simulated_env.reset()\n",
    "done = False\n",
    "state = torch.tensor(simulated_env.state).float()\n",
    "\n",
    "while not done:\n",
    "\n",
    "    # Store the current state of the CartPole for the video\n",
    "    video_recorder.capture_frame()\n",
    "\n",
    "    # Predict the state with the model\n",
    "    action = torch.tensor([simple_policy(state, p_random)])\n",
    "    state_action = torch.cat((state, action))\n",
    "    with torch.no_grad():\n",
    "        predicted_state = state_predictor(state_action)\n",
    "        predicted_state = list([float(s) for s in predicted_state.squeeze()])\n",
    "\n",
    "    # Apply it in the environment\n",
    "    state, reward, done, truncated, info = simulated_env.step(predicted_state)\n",
    "    state = torch.tensor(simulated_env.state).float()\n",
    "\n",
    "video_recorder.capture_frame()\n",
    "video_recorder.close()\n",
    "video_recorder.enabled = False\n",
    "\n",
    "simulated_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLriyAW593AR"
   },
   "source": [
    "### Question 6: Trying multiple loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbHSF0Bw934N"
   },
   "outputs": [],
   "source": [
    "# Alternative loss function\n",
    "\n",
    "def L1_loss(prediction, target):\n",
    "  \"\"\" \n",
    "  L1 loss function.\n",
    "  Input:\n",
    "    - prediction {torch.tensor} - target\n",
    "    - target {torch.tensor} - model prediction\n",
    "  Output: loss {float} - L1 error between the prediction and the target \n",
    "  \"\"\"\n",
    "  return (abs(prediction - target)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "ru5SHJDI-HDB",
    "outputId": "2899fffb-f02e-4deb-dd57-0e0f722e93a6"
   },
   "outputs": [],
   "source": [
    "## Torch MSE loss function\n",
    "torch_MSE_loss = nn.MSELoss()\n",
    "\n",
    "# You can call it the same way you would do with our hand design loss:\n",
    "inputs = batched_state_action[0] # Input of the model for this batch\n",
    "targets = batched_next_state[0] # Target output of the model for this batch\n",
    "loss = torch_MSE_loss(state_predictor(inputs), targets)\n",
    "print(loss)\n",
    "\n",
    "## Torch L1 loss function\n",
    "torch_L1_loss = nn.L1Loss()\n",
    "\n",
    "# You can call it the same way you would do with our hand design loss:\n",
    "inputs = batched_state_action[0] # Input of the model for this batch\n",
    "targets = batched_next_state[0] # Target output of the model for this batch\n",
    "loss = torch_L1_loss(state_predictor(inputs), targets)\n",
    "print(loss)\n",
    "\n",
    "## Torch Huber loss function\n",
    "torch_Huber_loss = nn.HuberLoss()\n",
    "\n",
    "# You can call it the same way you would do with our hand design loss:\n",
    "inputs = batched_state_action[0] # Input of the model for this batch\n",
    "targets = batched_next_state[0] # Target output of the model for this batch\n",
    "loss = torch_Huber_loss(state_predictor(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pl3dm2KOPrfi",
    "VG1q0h4MuOvp",
    "B7Tox87FUXwD",
    "NKHIOPm_UwK4",
    "6lBcIUZ1x6Bg",
    "0Bk-PIDvy3-o",
    "uLriyAW593AR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
